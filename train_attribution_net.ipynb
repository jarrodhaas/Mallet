{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TheHammer(nn.Module):\n",
    "    def __init__(self, num_hidden=2, dropout=0., in_feats=2048, num_classes=2):\n",
    "        super(TheHammer, self).__init__()\n",
    "        \n",
    "        self.extractor = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats//(l+1), in_feats//(l+2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for l in range(num_hidden)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_feats//(num_hidden+1), num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TheFuckingSledge(nn.Module):\n",
    "    def __init__(self, num_hidden=2, dropout=0., in_feats=2048, num_classes=2):\n",
    "        super(TheFuckingSledge, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv1d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avgpool = nn.AvgPool1d(2, stride=2)\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(2048, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x.unsqueeze(1))\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TheFuckingSledge(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv1d(1, 8, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (avgpool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (block4): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TheFuckingSledge(dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class TheHammerSack(Dataset):\n",
    "    def __init__(self, bag_of_tools: np.array, is_a_hammer: np.array, transform=None, target_transform=None):\n",
    "        super(TheHammerSack, self).__init__()\n",
    "        \n",
    "        self.bag_of_tools = bag_of_tools\n",
    "        self.is_a_hammer = is_a_hammer\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bag_of_tools)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        tool = self.bag_of_tools[ix]\n",
    "        is_this_a_hammer = self.is_a_hammer[ix]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            tool = self.transform(tool)\n",
    "        if self.target_transform is not None:\n",
    "            is_this_a_hammer = self.target_transform(is_this_a_hammer)\n",
    "            \n",
    "        return tool, is_this_a_hammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    " \n",
    "hammers_one = np.load(os.path.join('out', f'val-id_layer4.npy')).astype('float32')\n",
    "\n",
    "hammers_two = np.load(os.path.join('out', f'test-id_layer4.npy')).astype('float32')\n",
    "is_a_hammer_two = np.load(os.path.join('out', f'test-id_layer4_labels.npy')).astype('float32')\n",
    "hammers_two = np.concatenate((hammers_two[0, is_a_hammer_two == 0], hammers_two[1, is_a_hammer_two == 1]), axis=0)\n",
    "\n",
    "hammers = np.concatenate((hammers_one, hammers_two), axis=0)\n",
    "\n",
    "other_tools = np.load(os.path.join('out', f'test-ood_layer4.npy'))[0].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_distribution: \n",
      "hammers: 2300 400 300,\n",
      "other_tools: 2312 408 5280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hammers_train, hammers_test = train_test_split(hammers, test_size=0.1)\n",
    "hammers_train, hammers_val = train_test_split(hammers_train, test_size=0.1481)\n",
    "\n",
    "other_tools_train, other_tools_test = train_test_split(other_tools, test_size=0.66)\n",
    "other_tools_train, other_tools_val = train_test_split(other_tools_train, test_size=0.15)\n",
    "\n",
    "print(f\"data_distribution: \\n\"\n",
    "    f\"hammers: {len(hammers_train)} {len(hammers_val)} {len(hammers_test)},\\n\"\n",
    "    f\"other_tools: {len(other_tools_train)} {len(other_tools_val)} {len(other_tools_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_tools = np.concatenate((hammers_train, other_tools_train), axis=0)\n",
    "training_is_a_hammer = np.concatenate((np.ones(len(hammers_train)), np.zeros(len(other_tools_train))), axis=0).astype('int')\n",
    "\n",
    "shuffle_index = np.arange(len(training_tools))\n",
    "np.random.shuffle(shuffle_index)\n",
    "\n",
    "training_hammersack = TheHammerSack(training_tools[shuffle_index], training_is_a_hammer[shuffle_index])\n",
    "training_hammersack = DataLoader(training_hammersack, batch_size=64, pin_memory=True, drop_last=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tools = np.concatenate((hammers_val, other_tools_val), axis=0)\n",
    "validation_is_a_hammer = np.concatenate((np.ones(len(hammers_val)), np.zeros(len(other_tools_val))), axis=0).astype('int')\n",
    "\n",
    "shuffle_index = np.arange(len(validation_tools))\n",
    "np.random.shuffle(shuffle_index)\n",
    "\n",
    "validation_hammersack = TheHammerSack(validation_tools[shuffle_index], validation_is_a_hammer[shuffle_index])\n",
    "validation_hammersack = DataLoader(validation_hammersack, batch_size=64, pin_memory=True, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = 'cuda:0'\n",
    "# the_hammer = TheHammer(dropout=0.5).to(device)\n",
    "the_hammer = TheFuckingSledge(dropout=0.5).to(device)\n",
    "criterion = CrossEntropyLoss(reduction='mean').to(device)\n",
    "optimizer = SGD(params=the_hammer.parameters(), lr=0.01, weight_decay=5e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10, cooldown=10, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train: {'loss': 0.5010050159611114}\n",
      "train acc: 0.8115784908933218\n",
      "val: {'loss': 0.47321667808752793}\n",
      "val acc: 0.7871287128712872\n",
      "epoch: 1\n",
      "train: {'loss': 0.3459009795564495}\n",
      "train acc: 0.8594969644405898\n",
      "val: {'loss': 0.3605532508630019}\n",
      "val acc: 0.8452970297029703\n",
      "epoch: 2\n",
      "train: {'loss': 0.3148605086623806}\n",
      "train acc: 0.8594969644405898\n",
      "val: {'loss': 0.3579517121498401}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 3\n",
      "train: {'loss': 0.3044641001583779}\n",
      "train acc: 0.8647007805724197\n",
      "val: {'loss': 0.3244576293688554}\n",
      "val acc: 0.8551980198019802\n",
      "epoch: 4\n",
      "train: {'loss': 0.2991686024486202}\n",
      "train acc: 0.8668690372940157\n",
      "val: {'loss': 0.3221902916064629}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 5\n",
      "train: {'loss': 0.28753265207760953}\n",
      "train acc: 0.8683868169991327\n",
      "val: {'loss': 0.3178654003601808}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 6\n",
      "train: {'loss': 0.290733739733696}\n",
      "train acc: 0.8688204683434518\n",
      "val: {'loss': 0.5342272909787985}\n",
      "val acc: 0.745049504950495\n",
      "epoch: 7\n",
      "train: {'loss': 0.2816794024754877}\n",
      "train acc: 0.8705550737207285\n",
      "val: {'loss': 0.31446933631713575}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 8\n",
      "train: {'loss': 0.2780597158899046}\n",
      "train acc: 0.877059843885516\n",
      "val: {'loss': 0.31586598662229687}\n",
      "val acc: 0.8589108910891089\n",
      "epoch: 9\n",
      "train: {'loss': 0.2758565224605064}\n",
      "train acc: 0.877059843885516\n",
      "val: {'loss': 0.3164486667284599}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 10\n",
      "train: {'loss': 0.27314299202128633}\n",
      "train acc: 0.8779271465741544\n",
      "val: {'loss': 0.457783396427448}\n",
      "val acc: 0.7995049504950495\n",
      "epoch: 11\n",
      "train: {'loss': 0.27191355987770915}\n",
      "train acc: 0.880745880312229\n",
      "val: {'loss': 0.3125624702526973}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 12\n",
      "train: {'loss': 0.26991822131692544}\n",
      "train acc: 0.8820468343451865\n",
      "val: {'loss': 0.35080279409885406}\n",
      "val acc: 0.8304455445544554\n",
      "epoch: 13\n",
      "train: {'loss': 0.2620017042715255}\n",
      "train acc: 0.883347788378144\n",
      "val: {'loss': 0.3177901621048267}\n",
      "val acc: 0.844059405940594\n",
      "epoch: 14\n",
      "train: {'loss': 0.2659042953220132}\n",
      "train acc: 0.8818300086730269\n",
      "val: {'loss': 0.6120657485264999}\n",
      "val acc: 0.7079207920792079\n",
      "epoch: 15\n",
      "train: {'loss': 0.26029529065301976}\n",
      "train acc: 0.8813963573287077\n",
      "val: {'loss': 0.31231625148883235}\n",
      "val acc: 0.8613861386138614\n",
      "epoch: 16\n",
      "train: {'loss': 0.25825386729142435}\n",
      "train acc: 0.8892020815264527\n",
      "val: {'loss': 0.3138107107235835}\n",
      "val acc: 0.8465346534653465\n",
      "epoch: 17\n",
      "train: {'loss': 0.2543689363623319}\n",
      "train acc: 0.8909366869037294\n",
      "val: {'loss': 0.43847421499399036}\n",
      "val acc: 0.7858910891089109\n",
      "epoch: 18\n",
      "train: {'loss': 0.2525897193444918}\n",
      "train acc: 0.8883347788378144\n",
      "val: {'loss': 0.3205970858152096}\n",
      "val acc: 0.8477722772277227\n",
      "epoch: 19\n",
      "train: {'loss': 0.24674263167871188}\n",
      "train acc: 0.8931049436253252\n",
      "val: {'loss': 0.36024633049964905}\n",
      "val acc: 0.8304455445544554\n",
      "epoch: 20\n",
      "train: {'loss': 0.24519878758551322}\n",
      "train acc: 0.8954900260190807\n",
      "val: {'loss': 0.3392817171720358}\n",
      "val acc: 0.8329207920792079\n",
      "epoch: 21\n",
      "train: {'loss': 0.23895173877069395}\n",
      "train acc: 0.8933217692974849\n",
      "val: {'loss': 0.31443896660437953}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 22\n",
      "train: {'loss': 0.2353122554821511}\n",
      "train acc: 0.8987424111014745\n",
      "val: {'loss': 0.32337707854234254}\n",
      "val acc: 0.8403465346534653\n",
      "epoch: 23\n",
      "train: {'loss': 0.23487461174595847}\n",
      "train acc: 0.8965741543798785\n",
      "val: {'loss': 0.3278275831387593}\n",
      "val acc: 0.8477722772277227\n",
      "epoch: 24\n",
      "train: {'loss': 0.23297007535010167}\n",
      "train acc: 0.9011274934952298\n",
      "val: {'loss': 0.3286485637609775}\n",
      "val acc: 0.8391089108910891\n",
      "epoch: 25\n",
      "train: {'loss': 0.22924750308467917}\n",
      "train acc: 0.901561144839549\n",
      "val: {'loss': 0.3327115991940865}\n",
      "val acc: 0.8316831683168316\n",
      "epoch: 26\n",
      "train: {'loss': 0.2242433145029904}\n",
      "train acc: 0.9022116218560278\n",
      "val: {'loss': 0.3313769411582213}\n",
      "val acc: 0.8514851485148515\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "epoch: 27\n",
      "train: {'loss': 0.21832392068758402}\n",
      "train acc: 0.9065481352992194\n",
      "val: {'loss': 0.31403306928964764}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 28\n",
      "train: {'loss': 0.22034015292174194}\n",
      "train acc: 0.9098005203816132\n",
      "val: {'loss': 0.31297051218839794}\n",
      "val acc: 0.8564356435643564\n",
      "epoch: 29\n",
      "train: {'loss': 0.21499093938363742}\n",
      "train acc: 0.9078490893321769\n",
      "val: {'loss': 0.31374512956692624}\n",
      "val acc: 0.8564356435643564\n",
      "epoch: 30\n",
      "train: {'loss': 0.2142241809466114}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.3153609037399292}\n",
      "val acc: 0.8551980198019802\n",
      "epoch: 31\n",
      "train: {'loss': 0.2159653972477129}\n",
      "train acc: 0.9084995663486557\n",
      "val: {'loss': 0.316056951880455}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 32\n",
      "train: {'loss': 0.2210727380971386}\n",
      "train acc: 0.9069817866435386\n",
      "val: {'loss': 0.3155296719991244}\n",
      "val acc: 0.8576732673267327\n",
      "epoch: 33\n",
      "train: {'loss': 0.2132439229586353}\n",
      "train acc: 0.9076322636600174\n",
      "val: {'loss': 0.3148506788107065}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 34\n",
      "train: {'loss': 0.21588129172586415}\n",
      "train acc: 0.9095836947094535\n",
      "val: {'loss': 0.31430818942876965}\n",
      "val acc: 0.844059405940594\n",
      "epoch: 35\n",
      "train: {'loss': 0.2125452369043272}\n",
      "train acc: 0.9089332176929749\n",
      "val: {'loss': 0.3150477879322492}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 36\n",
      "train: {'loss': 0.2115952388881004}\n",
      "train acc: 0.9102341717259324\n",
      "val: {'loss': 0.31472415878222537}\n",
      "val acc: 0.8564356435643564\n",
      "epoch: 37\n",
      "train: {'loss': 0.2113637093403568}\n",
      "train acc: 0.9091500433651344\n",
      "val: {'loss': 0.3143354642849702}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 38\n",
      "train: {'loss': 0.21146331400903937}\n",
      "train acc: 0.9095836947094535\n",
      "val: {'loss': 0.3146353547389691}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 39\n",
      "train: {'loss': 0.2150028313267721}\n",
      "train acc: 0.9091500433651344\n",
      "val: {'loss': 0.31462811735960156}\n",
      "val acc: 0.8551980198019802\n",
      "epoch: 40\n",
      "train: {'loss': 0.2153192090049182}\n",
      "train acc: 0.910450997398092\n",
      "val: {'loss': 0.3192136883735657}\n",
      "val acc: 0.8477722772277227\n",
      "epoch: 41\n",
      "train: {'loss': 0.21230953380669634}\n",
      "train acc: 0.9095836947094535\n",
      "val: {'loss': 0.31511779817251057}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 42\n",
      "train: {'loss': 0.20927989544117287}\n",
      "train acc: 0.9091500433651344\n",
      "val: {'loss': 0.3157202899456024}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 43\n",
      "train: {'loss': 0.2123610925388663}\n",
      "train acc: 0.9124024284475282\n",
      "val: {'loss': 0.3137864630955916}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 44\n",
      "train: {'loss': 0.21467944469353925}\n",
      "train acc: 0.9106678230702515\n",
      "val: {'loss': 0.3141469703270839}\n",
      "val acc: 0.8564356435643564\n",
      "epoch: 45\n",
      "train: {'loss': 0.20957038122905444}\n",
      "train acc: 0.9100173460537727\n",
      "val: {'loss': 0.31573992050611055}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 46\n",
      "train: {'loss': 0.20760906936779414}\n",
      "train acc: 0.9098005203816132\n",
      "val: {'loss': 0.3146960735321045}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 47\n",
      "train: {'loss': 0.20675794147465326}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.3147770246634117}\n",
      "val acc: 0.8551980198019802\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "epoch: 48\n",
      "train: {'loss': 0.21593820395534985}\n",
      "train acc: 0.9106678230702515\n",
      "val: {'loss': 0.3148333556376971}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 49\n",
      "train: {'loss': 0.20545944244894263}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.3161790474103047}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 50\n",
      "train: {'loss': 0.20844373576445122}\n",
      "train acc: 0.9121856027753686\n",
      "val: {'loss': 0.3151133404328273}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 51\n",
      "train: {'loss': 0.20766092265305452}\n",
      "train acc: 0.9108846487424112\n",
      "val: {'loss': 0.31462170069034284}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 52\n",
      "train: {'loss': 0.20561252656864792}\n",
      "train acc: 0.9152211621856028\n",
      "val: {'loss': 0.3157389542231193}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 53\n",
      "train: {'loss': 0.20826791022738364}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.3156303201730435}\n",
      "val acc: 0.8564356435643564\n",
      "epoch: 54\n",
      "train: {'loss': 0.21063515482700035}\n",
      "train acc: 0.9128360797918473\n",
      "val: {'loss': 0.3139580889390065}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 55\n",
      "train: {'loss': 0.21003174332723226}\n",
      "train acc: 0.9115351257588898\n",
      "val: {'loss': 0.31685132017502415}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 56\n",
      "train: {'loss': 0.20554952915400676}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.31614997868354505}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 57\n",
      "train: {'loss': 0.21211971607926774}\n",
      "train acc: 0.9126192541196878\n",
      "val: {'loss': 0.31606391416146207}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 58\n",
      "train: {'loss': 0.20649627871709328}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.3154010084959177}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 59\n",
      "train: {'loss': 0.20699236337860968}\n",
      "train acc: 0.9102341717259324\n",
      "val: {'loss': 0.3143269121646881}\n",
      "val acc: 0.8477722772277227\n",
      "epoch: 60\n",
      "train: {'loss': 0.2061990879169882}\n",
      "train acc: 0.9124024284475282\n",
      "val: {'loss': 0.31548453867435455}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 61\n",
      "train: {'loss': 0.20677337387244996}\n",
      "train acc: 0.9108846487424112\n",
      "val: {'loss': 0.3146391946535844}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 62\n",
      "train: {'loss': 0.20770675852282405}\n",
      "train acc: 0.9124024284475282\n",
      "val: {'loss': 0.31437935622838825}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 63\n",
      "train: {'loss': 0.20726096027926222}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.3158893516430488}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 64\n",
      "train: {'loss': 0.20684842418317925}\n",
      "train acc: 0.9126192541196878\n",
      "val: {'loss': 0.31493120239331174}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 65\n",
      "train: {'loss': 0.20737688876178167}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.3163115210258044}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 66\n",
      "train: {'loss': 0.2081511448303314}\n",
      "train acc: 0.9089332176929749\n",
      "val: {'loss': 0.3142721148637625}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 67\n",
      "train: {'loss': 0.20819941244713247}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.3151749280782846}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 68\n",
      "train: {'loss': 0.20562750825734988}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.31548450887203217}\n",
      "val acc: 0.8502475247524752\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch: 69\n",
      "train: {'loss': 0.20853101707076374}\n",
      "train acc: 0.9108846487424112\n",
      "val: {'loss': 0.31614459363313824}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 70\n",
      "train: {'loss': 0.20675292917310376}\n",
      "train acc: 0.9124024284475282\n",
      "val: {'loss': 0.3142206026957585}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 71\n",
      "train: {'loss': 0.2069909998202977}\n",
      "train acc: 0.9117519514310495\n",
      "val: {'loss': 0.31583280632129085}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 72\n",
      "train: {'loss': 0.20716407466424655}\n",
      "train acc: 0.9121856027753686\n",
      "val: {'loss': 0.31592728197574615}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 73\n",
      "train: {'loss': 0.2091254708293366}\n",
      "train acc: 0.9108846487424112\n",
      "val: {'loss': 0.3155431804748682}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 74\n",
      "train: {'loss': 0.2091482163292088}\n",
      "train acc: 0.9111014744145707\n",
      "val: {'loss': 0.31461986899375916}\n",
      "val acc: 0.8551980198019802\n",
      "epoch: 75\n",
      "train: {'loss': 0.20829885024322223}\n",
      "train acc: 0.9115351257588898\n",
      "val: {'loss': 0.3153034666409859}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 76\n",
      "train: {'loss': 0.2093336473997325}\n",
      "train acc: 0.9095836947094535\n",
      "val: {'loss': 0.31446047127246857}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 77\n",
      "train: {'loss': 0.20801970444313467}\n",
      "train acc: 0.9137033824804857\n",
      "val: {'loss': 0.31449884405502904}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 78\n",
      "train: {'loss': 0.20644702997109662}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.31529165689761823}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 79\n",
      "train: {'loss': 0.2072178516281794}\n",
      "train acc: 0.9143538594969645\n",
      "val: {'loss': 0.3160091076905911}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 80\n",
      "train: {'loss': 0.20833102276880447}\n",
      "train acc: 0.9115351257588898\n",
      "val: {'loss': 0.31470126715990215}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 81\n",
      "train: {'loss': 0.2051007750712029}\n",
      "train acc: 0.9132697311361665\n",
      "val: {'loss': 0.31583243723099047}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 82\n",
      "train: {'loss': 0.20557503208313901}\n",
      "train acc: 0.9117519514310495\n",
      "val: {'loss': 0.3147811775024121}\n",
      "val acc: 0.8551980198019802\n",
      "epoch: 83\n",
      "train: {'loss': 0.20811185873534582}\n",
      "train acc: 0.9132697311361665\n",
      "val: {'loss': 0.3151385233952449}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 84\n",
      "train: {'loss': 0.20652618412285634}\n",
      "train acc: 0.9108846487424112\n",
      "val: {'loss': 0.3144093327797376}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 85\n",
      "train: {'loss': 0.20540314800527}\n",
      "train acc: 0.9134865568083261\n",
      "val: {'loss': 0.3158507656592589}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 86\n",
      "train: {'loss': 0.20828200448049258}\n",
      "train acc: 0.9121856027753686\n",
      "val: {'loss': 0.3152749893757013}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 87\n",
      "train: {'loss': 0.2108562949993839}\n",
      "train acc: 0.9128360797918473\n",
      "val: {'loss': 0.3151828795671463}\n",
      "val acc: 0.849009900990099\n",
      "epoch: 88\n",
      "train: {'loss': 0.2078636890813096}\n",
      "train acc: 0.9106678230702515\n",
      "val: {'loss': 0.3150480928329321}\n",
      "val acc: 0.8539603960396039\n",
      "epoch: 89\n",
      "train: {'loss': 0.2067291305897987}\n",
      "train acc: 0.9111014744145707\n",
      "val: {'loss': 0.3144907527245008}\n",
      "val acc: 0.8527227722772277\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch: 90\n",
      "train: {'loss': 0.20936206432238016}\n",
      "train acc: 0.9137033824804857\n",
      "val: {'loss': 0.3152510076761246}\n",
      "val acc: 0.8514851485148515\n",
      "epoch: 91\n",
      "train: {'loss': 0.20797380080370054}\n",
      "train acc: 0.9143538594969645\n",
      "val: {'loss': 0.3149411609539619}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 92\n",
      "train: {'loss': 0.20809872856695358}\n",
      "train acc: 0.9115351257588898\n",
      "val: {'loss': 0.3155132428957866}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 93\n",
      "train: {'loss': 0.20674606739249948}\n",
      "train acc: 0.9141370338248048\n",
      "val: {'loss': 0.31566502726995027}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 94\n",
      "train: {'loss': 0.20919574552202877}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.3164217345989667}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 95\n",
      "train: {'loss': 0.20832219189160492}\n",
      "train acc: 0.913052905464007\n",
      "val: {'loss': 0.31837157675853145}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 96\n",
      "train: {'loss': 0.2066084730298552}\n",
      "train acc: 0.9091500433651344\n",
      "val: {'loss': 0.31519094797281116}\n",
      "val acc: 0.8502475247524752\n",
      "epoch: 97\n",
      "train: {'loss': 0.2078966892337146}\n",
      "train acc: 0.9128360797918473\n",
      "val: {'loss': 0.31464380484360915}\n",
      "val acc: 0.8477722772277227\n",
      "epoch: 98\n",
      "train: {'loss': 0.20541934332210723}\n",
      "train acc: 0.911968777103209\n",
      "val: {'loss': 0.31372049107001376}\n",
      "val acc: 0.8527227722772277\n",
      "epoch: 99\n",
      "train: {'loss': 0.2078323539805739}\n",
      "train acc: 0.9117519514310495\n",
      "val: {'loss': 0.3157010685939055}\n",
      "val acc: 0.8502475247524752\n"
     ]
    }
   ],
   "source": [
    "from inferno.utils.train_utils import AverageMeter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(100):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    the_hammer.train()\n",
    "    train_trackers = defaultdict(AverageMeter)\n",
    "    all_labels, all_preds = [], []\n",
    "    for tools, is_it_a_hammer in training_hammersack:\n",
    "        tools = tools.to(device)\n",
    "        is_it_a_hammer = is_it_a_hammer.to(device)\n",
    "        \n",
    "        logits = the_hammer(tools)\n",
    "        loss = criterion(logits, is_it_a_hammer)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_labels.extend(is_it_a_hammer.cpu().detach().numpy().tolist())\n",
    "        all_preds.extend(logits.argmax(dim=1).detach().cpu().numpy().tolist())\n",
    "        train_trackers['loss'].update(loss.item())\n",
    "        \n",
    "    print('train: {}'.format({k: v.avg for k, v in train_trackers.items()}))\n",
    "    print(f'train acc: {classification_report(y_true=all_labels, y_pred=all_preds, output_dict=True)[\"accuracy\"]}')\n",
    "        \n",
    "    the_hammer.eval()\n",
    "    val_trackers = defaultdict(AverageMeter)\n",
    "    all_labels, all_preds = [], []\n",
    "    for tools, is_it_a_hammer in validation_hammersack:\n",
    "        tools = tools.to(device)\n",
    "        is_it_a_hammer = is_it_a_hammer.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = the_hammer(tools)\n",
    "            \n",
    "        loss = criterion(logits, is_it_a_hammer)\n",
    "        \n",
    "        all_labels.extend(is_it_a_hammer.cpu().detach().numpy().tolist())\n",
    "        all_preds.extend(logits.argmax(dim=1).detach().cpu().numpy().tolist())\n",
    "        val_trackers['loss'].update(loss.item())\n",
    "    \n",
    "    print('val: {}'.format({k: v.avg for k, v in val_trackers.items()}))\n",
    "    print(f'val acc: {classification_report(y_true=all_labels, y_pred=all_preds, output_dict=True)[\"accuracy\"]}')\n",
    "    \n",
    "    val_loss = val_trackers['loss'].avg\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(the_hammer.state_dict(), 'sledge_best.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hammersack",
   "language": "python",
   "name": "hammersack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
